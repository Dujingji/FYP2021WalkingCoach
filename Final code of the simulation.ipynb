{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6449443b",
   "metadata": {},
   "source": [
    "FYP 2021 S2 \n",
    "Jingji Du\n",
    "Walking coach 2D simulation and RL network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e506c6d",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7218a397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: Qt5Agg\n"
     ]
    }
   ],
   "source": [
    "#set up of the package \n",
    "# the package need \n",
    "# Ray, Tensorflow, gym, numpy\n",
    "\n",
    "\n",
    "%matplotlib\n",
    "\n",
    "import numpy as np \n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation \n",
    "import matplotlib.animation as animation \n",
    "\n",
    "import math\n",
    "\n",
    "import logging\n",
    "from typing import List, Optional, Type\n",
    "\n",
    "import logging\n",
    "from typing import List, Optional, Type\n",
    "\n",
    "import ray \n",
    "\n",
    "from ray.rllib.agents import ppo,ars\n",
    "\n",
    "from ray.rllib.agents.dqn.dqn_tf_policy import DQNTFPolicy\n",
    "from ray.rllib.agents.dqn.dqn_torch_policy import DQNTorchPolicy\n",
    "from ray.rllib.agents.dqn.simple_q import SimpleQTrainer, \\\n",
    "    DEFAULT_CONFIG as SIMPLEQ_DEFAULT_CONFIG\n",
    "from ray.rllib.agents.trainer import Trainer\n",
    "from ray.rllib.evaluation.worker_set import WorkerSet\n",
    "from ray.rllib.execution.concurrency_ops import Concurrently\n",
    "from ray.rllib.execution.metric_ops import StandardMetricsReporting\n",
    "from ray.rllib.execution.replay_buffer import LocalReplayBuffer\n",
    "from ray.rllib.execution.replay_ops import Replay, StoreToReplayBuffer\n",
    "from ray.rllib.execution.rollout_ops import ParallelRollouts\n",
    "from ray.rllib.execution.train_ops import TrainOneStep, UpdateTargetNetwork, \\\n",
    "    MultiGPUTrainOneStep\n",
    "from ray.rllib.policy.policy import LEARNER_STATS_KEY, Policy\n",
    "from ray.rllib.utils.typing import TrainerConfigDict\n",
    "from ray.util.iter import LocalIterator\n",
    "\n",
    "import gym\n",
    "from gym import Env\n",
    "from gym.spaces import Discrete, Box\n",
    "\n",
    "import tensorflow\n",
    "import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Flatten,Conv2D,Conv3D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from rl.agents import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "\n",
    "import ray\n",
    "import os\n",
    "from ray import tune\n",
    "from ray.rllib import agents\n",
    "from ray.tune.registry import register_env\n",
    "from itertools import count\n",
    "import pprint\n",
    "\n",
    "\n",
    "\n",
    "global inital_energy\n",
    "global inital_speed\n",
    "\n",
    "\n",
    "global Goal\n",
    "global person_speed_rate\n",
    "global person_moving_rate\n",
    "global Action_space \n",
    "\n",
    "# global variable \n",
    "person_speed_rate = {\"a\" : 0.8, \"b\" :0.7 , \"c\" : 0.95, \"d\" : 0.55, \"e\" : 0.65}\n",
    "person_moving_rate = {\"a\" : 0.5, \"b\" :0.6 , \"c\" : 0.3, \"d\" : 0.4, \"e\" : 0.7}\n",
    "Action_space = [\"A\",\"B\",\"C\",\"D\",\"E\"]\n",
    "Goal = 500\n",
    "\n",
    "Reward_epic = []\n",
    "Energy_epic = []\n",
    "Speed_epic = []\n",
    "\n",
    "global Reward_epic\n",
    "global Energy_epic\n",
    "global Speed_epic\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19124c26",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e09d98c",
   "metadata": {},
   "source": [
    "Here are the Dictionary used for Personal type and Orientation.\n",
    "\n",
    "The Person type defines The impact of the action. \n",
    "\n",
    "Example:\n",
    "If the robot pickup action A for Person A, the energy of person A will increase 40 and orientaton will increse 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9945a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "Person_a = {\"A\" : 40, \"B\": -25, \"C\" : -10, \"D\":15, \"E\" :-5}\n",
    "Person_b = {\"A\" : 35, \"B\": -20, \"C\" :-15, \"D\":22, \"E\" :-3}\n",
    "Person_c = {\"A\" : -37, \"B\": -10, \"C\" :-15, \"D\":20, \"E\" :15}\n",
    "Person_d = {\"A\" : -21, \"B\": 35, \"C\" :-14, \"D\":10, \"E\" :-7}\n",
    "Person_e = {\"A\" : 29, \"B\": -23, \"C\" :-19, \"D\":25, \"E\" :-10}\n",
    "\n",
    "person_dic = {\"a\" : Person_a, \"b\" : Person_b, \"c\" : Person_c, \"d\" : Person_d, \"e\" : Person_e}\n",
    "\n",
    "Orientation_a = {\"A\" : 1, \"B\": -3, \"C\" : 5, \"D\":-3, \"E\" :-2}\n",
    "Orientation_b  = {\"A\" : 1, \"B\": -2, \"C\" :3, \"D\":-1, \"E\" :1}\n",
    "Orientation_c = {\"A\" : 2, \"B\": -3, \"C\" :3, \"D\":-3, \"E\" :-1}\n",
    "Orientation_d  = {\"A\" : 2, \"B\": -5, \"C\" :4, \"D\":-1, \"E\" :0}\n",
    "Orientation_e  = {\"A\" : 3, \"B\": -2, \"C\" :4, \"D\":-2, \"E\" :2}\n",
    "\n",
    "orientation_dic = {\"a\" : Orientation_a, \"b\" : Orientation_b, \"c\" : Orientation_c, \"d\" :Orientation_d, \"e\" : Orientation_e}\n",
    "\n",
    "Distance_dic = {\"I\":0.45,\"Pe\":1.2,\"S\":3.6,\"Pu\":7.6}\n",
    "\n",
    "#figure setting \n",
    "#fig1 = plt.figure(1)\n",
    "#ax = plt.axes(xlim =(0,100),ylim = (0,1000))\n",
    "#ax.set_aspect(\"equal\")\n",
    "#fig2 = plt.figure(2)\n",
    "#ax = plt.axes(xlim =(0,30),ylim = (0,100))\n",
    "# set equal aspect such that the circle is not shown as ellipse\n",
    "\n",
    "x_vals = []\n",
    "y_vals = []\n",
    "energy_vals = []\n",
    "energyX_vals = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6d856b",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c125103",
   "metadata": {},
   "source": [
    "Random function generation and energy decrease function \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e56029ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ramdonGenerator():\n",
    "    return random.randint(30,80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe5e03f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ramdonGenerator2():\n",
    "    return random.randint(0,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7cd53f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ramdonLocationGenerator():\n",
    "    return  random.randint(0,200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aca3dde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def energy_decay(personalType,step,person_speed_rate):\n",
    "    \n",
    "    decayRate = person_speed_rate[personalType]   \n",
    "    energy_decay = inital_energy - inital_energy * decayRate * np.exp(-step/40)*0.5\n",
    "    \n",
    "    return energy_decay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee70c46a",
   "metadata": {},
   "source": [
    "Person agent class \n",
    "\n",
    "Person Class has simulation person informaction:\n",
    "\n",
    "Energy, orientation, location,personal type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35640c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Person:\n",
    "    \n",
    "    def __init__(self,intital_energy,action_table,start_location_Y,start_location_X,person_Type,inital_speed,OrientationTable):\n",
    "        \n",
    "        self.init_energy = intital_energy\n",
    "        self.actions = action_table\n",
    "        self.cur_speed = inital_speed;\n",
    "        self.cur_energy = intital_energy\n",
    "        self.location_Y = start_location_Y\n",
    "        self.personType = person_Type\n",
    "        self.actionTable = action_table\n",
    "        \n",
    "        self.location_X = start_location_X\n",
    "        self.orientation = 90\n",
    "        self.OrientationTable = OrientationTable\n",
    "        self.walking_speed = 50\n",
    "        \n",
    "    def setSpeed(self):\n",
    "        walking_speed = self.walking_speed\n",
    "        if self.cur_energy < 10 or self.cur_energy > 90:\n",
    "            self.cur_speed = 0*walking_speed\n",
    "        elif self.cur_energy <= 60 and self.cur_energy >= 40:\n",
    "            self.cur_speed = -1/16*(40/10-1)*(40/10-9)*walking_speed\n",
    "        else:\n",
    "             self.cur_speed = -1/16*(self.cur_energy/10-1)*(self.cur_energy/10-9)*walking_speed\n",
    "                \n",
    "        \n",
    "        if self.cur_speed <= 0:\n",
    "            self.cur_speed = 0\n",
    "        if self.cur_speed >= walking_speed:\n",
    "            self.cur_speed = walking_speed\n",
    "     \n",
    "            \n",
    "        \n",
    "    def setEnergy(self,current_energy):\n",
    "        self.cur_energy = current_energy\n",
    "        if self.cur_energy > 100:\n",
    "            self.cur_energy = 100\n",
    "            \n",
    "        elif self.cur_energy < 0:\n",
    "            self.cur_energy = 0\n",
    "        \n",
    "        \n",
    "        \n",
    "     \n",
    "    def setLocation_Y(self):\n",
    "        \n",
    "        self.location_Y = self.location_Y + self.cur_speed*person_moving_rate[self.personType]*math.sin(self.orientation*math.pi/180)\n",
    "        \n",
    "        if self.location_Y >= Goal:\n",
    "            self.location_Y = Goal\n",
    "        elif self.location_Y <= 0:\n",
    "             self.location_Y = 0\n",
    "        \n",
    "        \n",
    "    def setLocation_X(self):\n",
    "        self.location_X = self.location_X + self.cur_speed*person_moving_rate[self.personType]*math.cos(self.orientation*math.pi/180)\n",
    "        \n",
    "        if self.location_X > 1000:\n",
    "            self.location_X = 1000\n",
    "        elif self.location_X < 0:\n",
    "            self.location_X = 0\n",
    "            \n",
    "    def setOrientation(self,orientation_change):\n",
    "    \n",
    "        self.orientation = self.orientation + orientation_change\n",
    "        \n",
    "        if self.orientation > 180:\n",
    "            self.orientation = -(360-self.orientation)\n",
    "        if self.orientation > 360:\n",
    "            self.orientation = self.orientation - 360\n",
    "        if self.orientation < -180:\n",
    "            self.orientation = 360 + self.orientation\n",
    "        if self.orientation < -360:\n",
    "            self.orientation = self.orientation + 360\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    def getSpeed(self):\n",
    "        return self.cur_speed \n",
    "    \n",
    "    def getEnergy(self):\n",
    "        return self.cur_energy\n",
    "    \n",
    "    def getLocation_Y(self):\n",
    "        return self.location_Y\n",
    "    def getLocation_X(self):\n",
    "        return self.location_X\n",
    "\n",
    "    def getOrientation(self):\n",
    "        return self.orientation\n",
    "    \n",
    "    \n",
    "    \n",
    "    def getPersonType(self):\n",
    "        return self.personType\n",
    "    \n",
    "    def getPersonNumber(self):\n",
    "        \n",
    "        list_t = {\"a\" : 0, \"b\" : 1, \"c\" : 2, \"d\" : 3, \"e\" : 4}\n",
    "        person_number = list_t[self.personType] \n",
    "        return person_number\n",
    "    \n",
    "    def getActionTable(self):\n",
    "        return self.actionTable\n",
    "    \n",
    "    def getOrientationTable(self):\n",
    "        return self.OrientationTable\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b18aa3",
   "metadata": {},
   "source": [
    "\n",
    "Robot agent class \n",
    "\n",
    "The robot agent only contain 2 information \n",
    "\n",
    "Location and the Soical distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d484d565",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Robort:\n",
    "    \n",
    "    def __init__(self,start_location_Y,start_location_X,distance_human):\n",
    "        \n",
    "        self.location_Y = start_location_Y\n",
    "        self.location_X = start_location_X\n",
    "        self.distance_human = distance_human\n",
    "        \n",
    "    def setLocation_Y(self,locationY):\n",
    "        self.location_Y = locationY\n",
    "        \n",
    "  \n",
    "    def setLocation_X(self,locationX):\n",
    "        self.location_X = locationX\n",
    "        \n",
    "    \n",
    "    def setdistance_human(self,distance_human):\n",
    "        self.distance_human = distance_human\n",
    "        \n",
    "    def getLocation_Y(self):\n",
    "        return self.location_Y\n",
    "    \n",
    "    def getLocation_X(self):\n",
    "        return self.location_X\n",
    "    \n",
    "    def getdistance_human(self):\n",
    "        return self.distance_human\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54cb9cf2",
   "metadata": {},
   "source": [
    "Here to generator a person agent to start the training and simulation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "133fe868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n"
     ]
    }
   ],
   "source": [
    "# inintal all the varible randomly\n",
    "\n",
    "inital_energy = ramdonGenerator()\n",
    "inital_speed =  ramdonGenerator()\n",
    "start_location_Y = ramdonLocationGenerator()\n",
    "start_location_X = 500\n",
    "start_distance_human = 5.0\n",
    "\n",
    "# enter person type: \"a,b,c,d,e\"\n",
    "PrsonType = input()\n",
    "Person_type = person_dic[PrsonType]\n",
    "orientationTable = orientation_dic[PrsonType]\n",
    "sim_Person = Person(inital_energy,Person_type,start_location_Y,start_location_X,PrsonType,inital_speed,orientationTable)\n",
    "sim_Robot = Robort(sim_Person.getLocation_Y(),sim_Person.getLocation_X(),start_distance_human)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02850d2",
   "metadata": {},
   "source": [
    "RL network for Soical norms traninig "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a605f83",
   "metadata": {},
   "outputs": [],
   "source": [
    " class SoicalNorm(gym.Env):\n",
    "    def __init__(self,env_config):\n",
    "        #Action we can take, A,B,C,D,E\n",
    "        self.action_space = Discrete(4)\n",
    "        #Energy array \n",
    "        self.observation_space = Box(low=np.array([0.0,0.0]),high = np.array([10.0,100.0]),dtype = np.float32)\n",
    "        #set start Enegry\n",
    "        self.state = [sim_Robot.getdistance_human(),sim_Person.getEnergy()]\n",
    "        self.Step = 0\n",
    "        \n",
    "        \n",
    "    def step(self, action):\n",
    "        \n",
    "     \n",
    "        \n",
    "  \n",
    "        \n",
    "        self.Step += 1\n",
    "        a_list = [\"I\",\"Pe\",\"S\",\"Pu\"]\n",
    "        \n",
    "        # The action range\n",
    "        # Example:\n",
    "        #if the action is 2:\n",
    "        #The robot will pick the range in 1.2m to 3.6m\n",
    "        \n",
    "        if action == 0:\n",
    "            range_pick_upper = Distance_dic[a_list[action]]\n",
    "            range_pick_lower  = 0\n",
    "        else:\n",
    "            range_pick_upper = Distance_dic[a_list[action]]\n",
    "            range_pick_lower = Distance_dic[a_list[action-1]]\n",
    "        \n",
    "        \n",
    "        #Reward \n",
    "        \n",
    "        reward = 0\n",
    "        if sim_Person.getEnergy() >= 40 and sim_Person.getEnergy() <= 60:\n",
    "            if action == 0:\n",
    "                reward = -100\n",
    "            elif action == 1:\n",
    "                reward = 100\n",
    "            elif action == 2:\n",
    "                reward = 10\n",
    "            elif action == 3:\n",
    "                reward = -10\n",
    "                \n",
    "        if sim_Person.getEnergy() < 40 and sim_Person.getEnergy() >= 10:\n",
    "            if action == 0:\n",
    "                reward = -100\n",
    "            elif action == 1:\n",
    "                reward = 10\n",
    "            elif action == 2:\n",
    "                reward = 100\n",
    "            elif action == 3:\n",
    "                reward = -10\n",
    "                \n",
    "        if sim_Person.getEnergy() > 60 and sim_Person.getEnergy() <= 90:\n",
    "            if action == 0:\n",
    "                reward = -100\n",
    "            elif action == 1:\n",
    "                reward = -10\n",
    "            elif action == 2:\n",
    "                reward = 10\n",
    "            elif action == 3:\n",
    "                reward = 100\n",
    "                \n",
    "        if sim_Person.getEnergy() < 10 and sim_Person.getEnergy() > 90:\n",
    "            if action == 0:\n",
    "                reward = -100\n",
    "            elif action == 1:\n",
    "                reward = -100\n",
    "            elif action == 2:\n",
    "                reward = -100\n",
    "            elif action == 3:\n",
    "                reward = 100\n",
    "        \n",
    "        # For the action range, generate a random number\n",
    "        \n",
    "        human_distance =  random.random() * (range_pick_upper -  range_pick_lower) +  range_pick_lower\n",
    "        sim_Robot.setdistance_human(human_distance)\n",
    "        \n",
    "        # End condition for one eposide\n",
    "        if self.Step <= 200: \n",
    "            done = False\n",
    "        else:\n",
    "            done = True\n",
    "        \n",
    "            \n",
    "        #if you are traning the network uncommond below code, if for test commond the below code:\n",
    "        \n",
    "        #sim_Person.setEnergy(ramdonGenerator2())\n",
    "        \n",
    "        \n",
    "        self.state = [sim_Robot.getdistance_human(),sim_Person.getEnergy()]\n",
    "\n",
    "        info = {}\n",
    "        \n",
    "        return self.state,reward,done,info  \n",
    "        \n",
    "    def reset(self):\n",
    "        inital_energy = ramdonGenerator()\n",
    "        inital_speed = inital_energy\n",
    "        start_location_Y = ramdonLocationGenerator()\n",
    "        start_location_X = 500\n",
    "        typePerson = random.randint(0,4)\n",
    "        personList = [\"a\",\"b\",\"c\",\"d\",\"e\"]\n",
    "        PrsonType = personList[typePerson]\n",
    "        Person_type = person_dic[PrsonType]\n",
    "        orientationTable = orientation_dic[PrsonType]\n",
    "        start_distance_human = 3.6\n",
    "        sim_Person = Person(inital_energy,Person_type,start_location_Y,start_location_X,PrsonType,inital_speed,orientationTable)\n",
    "        sim_Robot = Robort(sim_Person.getLocation_Y(),sim_Person.getLocation_X(),start_distance_human)\n",
    "        \n",
    "        self.state = [sim_Robot.getdistance_human(),sim_Person.getEnergy()]\n",
    "        \n",
    "        return self.state\n",
    "    \n",
    "    def render():\n",
    "        return\n",
    "    \n",
    "    def setTeststate(self,energy):\n",
    "        self.state = [3.6,energy]\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c400b5d3",
   "metadata": {},
   "source": [
    "Here are the agent class tester to see if the agent is working or not\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4811c5ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2\n",
      "0.5602829252698659\n",
      "0.5602829252698659\n",
      "52\n"
     ]
    }
   ],
   "source": [
    "a_list = [\"I\",\"Pe\",\"S\",\"Pu\"]\n",
    "action = 1\n",
    "if action == 0:\n",
    "            range_pick_upper = Distance_dic[a_list[action]]\n",
    "            range_pick_lower  = 0\n",
    "else:\n",
    "            range_pick_upper = Distance_dic[a_list[action]]\n",
    "            range_pick_lower = Distance_dic[a_list[action-1]]\n",
    "            \n",
    "human_distance = random.random() * (range_pick_upper -  range_pick_lower) +  range_pick_lower;\n",
    "print(range_pick_upper)\n",
    "sim_Robot.setdistance_human(human_distance)\n",
    "print(sim_Robot.getdistance_human())\n",
    "\n",
    "print(human_distance)\n",
    "sim_Person.setEnergy(ramdonGenerator())\n",
    "print(sim_Person.getEnergy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8e925c",
   "metadata": {},
   "source": [
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89df0e44",
   "metadata": {},
   "source": [
    "Here start the set up for SoicalNorm RL traning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "44497195",
   "metadata": {},
   "outputs": [],
   "source": [
    "register_env(\"SoicalNorm\", lambda config:SoicalNorm(config))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab3d7c9",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f874141b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer =ppo.PPOTrainer(env = \"SoicalNorm\")\n",
    "\n",
    "N_ITER = 10\n",
    "pp = pprint.PrettyPrinter()\n",
    "\n",
    "for n in range(N_ITER):\n",
    "    result = trainer.train()\n",
    "    #pprint.pprint(result)\n",
    "    file_name = trainer.save()\n",
    "    print(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5705d4ee",
   "metadata": {},
   "source": [
    "Reload the trained network \n",
    "\n",
    "for checkpoint_path plz replace the path where your trained ntwork saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c2ba8c74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=27632)\u001b[0m C:\\Users\\jdu39\\anaconda3\\lib\\site-packages\\gym\\logger.py:34: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=27632)\u001b[0m   warnings.warn(colorize(\"%s: %s\" % (\"WARN\", msg % args), \"yellow\"))\n",
      "\u001b[2m\u001b[36m(pid=45764)\u001b[0m C:\\Users\\jdu39\\anaconda3\\lib\\site-packages\\gym\\logger.py:34: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=45764)\u001b[0m   warnings.warn(colorize(\"%s: %s\" % (\"WARN\", msg % args), \"yellow\"))\n",
      "2021-10-22 19:56:20,688\tINFO trainable.py:106 -- Trainable.setup took 21.520 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(pid=33868)\u001b[0m Windows fatal exception: access violation\n",
      "\u001b[2m\u001b[36m(pid=33868)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=10124)\u001b[0m Windows fatal exception: access violation\n",
      "\u001b[2m\u001b[36m(pid=10124)\u001b[0m \n",
      "2021-10-22 19:56:20,801\tINFO trainable.py:382 -- Restored on 192.168.0.115 from checkpoint: C:\\Users\\jdu39/ray_results\\PPO_SoicalNorm_2021-10-14_20-35-52w1qb82bu\\checkpoint_000010\\checkpoint-10\n",
      "2021-10-22 19:56:20,802\tINFO trainable.py:390 -- Current state after restoring: {'_iteration': 10, '_timesteps_total': None, '_time_total': 181.48614263534546, '_episodes_total': 39600}\n"
     ]
    }
   ],
   "source": [
    "agent_social = ppo.PPOTrainer(env = \"SoicalNorm\")\n",
    "checkpoint_path = r\"C:\\Users\\jdu39/ray_results\\PPO_SoicalNorm_2021-10-14_20-35-52w1qb82bu\\checkpoint_000010\\checkpoint-10\"\n",
    "agent_social.restore(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eaa6f07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "97fa5bd3",
   "metadata": {},
   "source": [
    "\n",
    "The main RL network \n",
    "\n",
    "It is use to train and test RL agent to pick a action to keep energy and orientation in a desired range. \n",
    "\n",
    "The action space is Discrete (0,1,2,3,4,5)\n",
    "\n",
    "The observation space is continuous box space for 4 states: energy, orientation,speed and personal type\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1e9d64d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyEnv(gym.Env):\n",
    "    def __init__(self,env_config):\n",
    "        #Action we can take, A,B,C,D,E\n",
    "        self.action_space = Discrete(5)\n",
    "        #Energy array \n",
    "        self.observation_space = Box(low= np.array([0,0,-180,0]),high = np.array([100,100,180,4]),dtype = np.float64)\n",
    "        #set start Enegry\n",
    "        self.state = [sim_Person.getEnergy(),sim_Person.getSpeed(),sim_Person.getOrientation(),sim_Person.getPersonNumber()]\n",
    "        \n",
    "        self.old_state = self.state\n",
    "        #set goal\n",
    "        \n",
    "        self.goal = Goal\n",
    "        self.goaldistance = Goal\n",
    "        self.Step = 0\n",
    "        \n",
    "        self.person = sim_Person\n",
    "        \n",
    "    \n",
    "        \n",
    "    def step(self, action):\n",
    "        #step in each iteration \n",
    "        \n",
    "        self.Step += 1\n",
    "        \n",
    "        #apply action \n",
    "        a_list = [\"A\",\"B\",\"C\",\"D\",\"E\"]\n",
    "        \n",
    "        # energy decay each step\n",
    "        #energy_change = self.state[0]-self.state[1]/10\n",
    "        #self.person.setEnergy(energy_change)\n",
    "        self.state[0] = self.person.getEnergy()\n",
    "        \n",
    "        \n",
    "        \n",
    "        #energy change after action\n",
    "        \n",
    "        Person_type = self.person.getActionTable()\n",
    "        orientation_type = self.person.getOrientationTable()\n",
    "        \n",
    "        \n",
    "        orientation_change = orientation_type[a_list[action]]\n",
    "        energy_change = self.state[0]+Person_type[a_list[action]]\n",
    "        self.person.setEnergy(energy_change)\n",
    "        self.state[0] = self.person.getEnergy()\n",
    "        \n",
    "    \n",
    "\n",
    "        \n",
    "        #update the person class \n",
    "        self.person.setSpeed()\n",
    "        self.person.setOrientation(orientation_change)\n",
    "        self.person.setLocation_Y()\n",
    "        self.person.setLocation_X()\n",
    "        \n",
    "        \n",
    "         # how far to the goal\n",
    "        self.goaldistance = self.goal - self.person.getLocation_Y()\n",
    "        \n",
    "        #set point in figure \n",
    "        # create a point in the axes\n",
    "\n",
    "        self.state[1] = self.person.getSpeed()\n",
    "        self.state[2] = self.person.getOrientation()\n",
    "        self.state[3] = self.person.getPersonNumber()\n",
    "        \n",
    "      \n",
    "        \n",
    "        #calculating reward\n",
    "        # reward function is desirbe in the final report. \n",
    "        \n",
    "        reward = 0\n",
    "        speed_reward = 0\n",
    "        \n",
    "        max_reward = 35\n",
    "        min_reward = -50\n",
    "        scale_factor = 10\n",
    "        \n",
    "        if (self.state[0] <= 60 and self.state[0] >= 40) and (self.state[2] <= 95 and self.state[2] >= 85):\n",
    "            reward = max_reward \n",
    "            speed_reward = self.state[1]/46.875 * max_reward\n",
    "        elif (self.state[0] < 10 or self.state[0] > 90) and (self.state[2] < 45  or self.state[2] > 135):\n",
    "            reward = min_reward\n",
    "            speed_reward = 0\n",
    "            \n",
    "        elif (self.state[0] < 10 or self.state[0] > 90) or (self.state[2] < 45  or self.state[2] > 135):\n",
    "            if (self.state[0] < 10 or self.state[0] > 90) and (self.state[2] > 45 and self.state[2] < 135):\n",
    "                reward = min_reward*(1+1/20.25*(self.state[2]/scale_factor-4.5)*(self.state[2]/scale_factor-13.5))\n",
    "                speed_reward = 0\n",
    "            elif (self.state[2] < 45 or self.state[2] > 135) and (self.state[0] > 10 and self.state[0] < 90):\n",
    "                reward = min_reward*(1+1/16*(self.state[0]/scale_factor-1)*(self.state[0]/scale_factor-9))\n",
    "                speed_reward = self.state[1]/46.875* max_reward\n",
    "        else:\n",
    "            reward = -(self.state[0]/scale_factor-1)*(self.state[0]/scale_factor-9) - (self.state[2]/scale_factor-4.5)*(self.state[2]/scale_factor-13.5)\n",
    "            speed_reward = self.state[1]/46.875* max_reward\n",
    "            \n",
    "        reward += speed_reward \n",
    "\n",
    "        \n",
    "        #check if the walking done\n",
    "        Timeout = False\n",
    "        done = False\n",
    "        if  self.goaldistance <= 0:\n",
    "            done = True\n",
    "        elif self.Step > 50:\n",
    "            done = True\n",
    "            Timeout = True\n",
    "        \n",
    "                \n",
    "        info = {}\n",
    "        \n",
    " \n",
    "        \n",
    "        \n",
    "        \n",
    "        self.old_state = self.state\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        return self.state,reward,done,info   \n",
    "        \n",
    "    def reset(self):\n",
    "        \n",
    "        #reset all the parameter \n",
    "        inital_speed = ramdonGenerator()\n",
    "        inital_energy = ramdonGenerator()\n",
    "        start_location_Y = ramdonLocationGenerator()\n",
    "        start_location_X = 500\n",
    "        typePerson = random.randint(0,4)\n",
    "        personList = [\"a\",\"b\",\"c\",\"d\",\"e\"]\n",
    "        PrsonType = personList[typePerson]\n",
    "        Person_type = person_dic[PrsonType]\n",
    "        orientationTable = orientation_dic[PrsonType]\n",
    "        \n",
    "        sim_Person = Person(inital_energy,Person_type,start_location_Y,start_location_X,PrsonType,inital_speed,orientationTable)\n",
    "        \n",
    "\n",
    "        self.state = [sim_Person.getEnergy(),sim_Person.getSpeed(),sim_Person.getOrientation(),sim_Person.getPersonNumber()]\n",
    "        self.goal = Goal\n",
    "        self.goaldistance = Goal\n",
    "        self.Step = 0\n",
    "        \n",
    "        self.person = sim_Person\n",
    "        self.old_state = self.state\n",
    "        \n",
    "            \n",
    "        return self.state\n",
    "        \n",
    "        \n",
    "        \n",
    "    def render(self):\n",
    "        return \n",
    "        \n",
    "    def getPerson(self):\n",
    "        return self.person "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1366c0ad",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2a696b",
   "metadata": {},
   "source": [
    "To inital ray library "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a58be292",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jdu39\\anaconda3\\lib\\site-packages\\ray\\_private\\services.py:238: UserWarning: Not all Ray Dashboard dependencies were found. To use the dashboard please install Ray using `pip install ray[default]`. To disable this message, set RAY_DISABLE_IMPORT_WARNING env var to '1'.\n",
      "  warnings.warn(warning_message)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': '192.168.0.115',\n",
       " 'raylet_ip_address': '192.168.0.115',\n",
       " 'redis_address': '192.168.0.115:6379',\n",
       " 'object_store_address': 'tcp://127.0.0.1:59891',\n",
       " 'raylet_socket_name': 'tcp://127.0.0.1:52799',\n",
       " 'webui_url': None,\n",
       " 'session_dir': 'C:\\\\Users\\\\jdu39\\\\AppData\\\\Local\\\\Temp\\\\ray\\\\session_2021-10-22_19-30-46_152329_43568',\n",
       " 'metrics_export_port': 56903,\n",
       " 'node_id': 'a8bfbdb4bb3fca4256b803e3203b04a55346c0e13d81eb974ef6886d'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ray.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701377d9",
   "metadata": {},
   "source": [
    "To shutdown ray library "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a11050d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#uncommond it if you want to shutdown ray \n",
    "\n",
    "#ray.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ba9fee",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce62696",
   "metadata": {},
   "source": [
    "training Set up for main RL agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "66a400d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "register_env(\"MyEnv\", lambda config:MyEnv(config))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6036f93",
   "metadata": {},
   "source": [
    "The policy network sturcture can be found here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9618159c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-17559eebb084>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpolicy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_policy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'trainer' is not defined"
     ]
    }
   ],
   "source": [
    "policy = trainer.get_policy()\n",
    "model = policy.model\n",
    "print(model.base_model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2190a617",
   "metadata": {},
   "source": [
    "Traning agent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eed10cc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "trainer =ppo.PPOTrainer(env = \"MyEnv\")\n",
    "\n",
    "index = count()\n",
    "\n",
    "N_ITER = 30\n",
    "pp = pprint.PrettyPrinter()\n",
    "\n",
    "for n in range(N_ITER):\n",
    "    result = trainer.train()\n",
    "    #pprint.pprint(result)\n",
    "    file_name = trainer.save()\n",
    "    print(file_name)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44644ea7",
   "metadata": {},
   "source": [
    "Reload the trained network\n",
    "\n",
    "for checkpoint_path plz replace the path where your trained network saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "40411324",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=22108)\u001b[0m C:\\Users\\jdu39\\anaconda3\\lib\\site-packages\\gym\\logger.py:34: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float64\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=22108)\u001b[0m   warnings.warn(colorize(\"%s: %s\" % (\"WARN\", msg % args), \"yellow\"))\n",
      "\u001b[2m\u001b[36m(pid=24060)\u001b[0m C:\\Users\\jdu39\\anaconda3\\lib\\site-packages\\gym\\logger.py:34: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float64\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=24060)\u001b[0m   warnings.warn(colorize(\"%s: %s\" % (\"WARN\", msg % args), \"yellow\"))\n",
      "2021-10-22 19:57:04,345\tINFO trainable.py:106 -- Trainable.setup took 19.899 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(pid=33124)\u001b[0m Windows fatal exception: access violation\n",
      "\u001b[2m\u001b[36m(pid=33124)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=11104)\u001b[0m Windows fatal exception: access violation\n",
      "\u001b[2m\u001b[36m(pid=11104)\u001b[0m \n",
      "2021-10-22 19:57:04,504\tINFO trainable.py:382 -- Restored on 192.168.0.115 from checkpoint: C:\\Users\\jdu39/ray_results\\PPO_MyEnv_2021-10-15_14-34-30ozo1cjc6\\checkpoint_000030\\checkpoint-30\n",
      "2021-10-22 19:57:04,508\tINFO trainable.py:390 -- Current state after restoring: {'_iteration': 30, '_timesteps_total': None, '_time_total': 156.2457344532013, '_episodes_total': 4458}\n"
     ]
    }
   ],
   "source": [
    "agent = ppo.PPOTrainer(env = \"MyEnv\")\n",
    "checkpoint_path = r\"C:\\Users\\jdu39/ray_results\\PPO_MyEnv_2021-10-15_14-34-30ozo1cjc6\\checkpoint_000030\\checkpoint-30\"\n",
    "agent.restore(checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e24b0ba",
   "metadata": {},
   "source": [
    "Here is for testing \n",
    "\n",
    "remember to commonet out the RandomGeneration() in the soical norm network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e2de8009",
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing \n",
    "\n",
    "Goal = 500\n",
    "Reward_epic = []\n",
    "Energy_epic = []\n",
    "Speed_epic = []\n",
    "orientation_epic = []\n",
    "number_epic = []\n",
    "Location_X_epic = []\n",
    "Location_Y_epic = []\n",
    "episode_reward_epic = []\n",
    "soical_reward_epic = []\n",
    "human_distance_epic = []\n",
    "\n",
    "\n",
    "env = MyEnv({})\n",
    "social_env = SoicalNorm({})\n",
    "\n",
    "\n",
    "# run until episode ends\n",
    "episode_reward = 0\n",
    "step = 0\n",
    "done = False\n",
    "obs = env.reset()\n",
    "soical_obs = [3.6,obs[0]]\n",
    "#print(obs)\n",
    "#print(soical_obs)\n",
    "\n",
    "for i in range(50):\n",
    "    while not done:\n",
    "        action = agent.compute_single_action(obs)\n",
    "        action_soical = agent_social.compute_single_action(soical_obs)\n",
    "        \n",
    "        obs, reward, done, info = env.step(action)\n",
    "        soical_obs, soical_reward,d,i = social_env.step(action_soical)\n",
    "        \n",
    "        person_test = env.getPerson()\n",
    "        Location_X = person_test.getLocation_X()\n",
    "        Location_X_epic.append(Location_X)\n",
    "        \n",
    "        Location_Y = person_test.getLocation_Y()\n",
    "        Location_Y_epic.append(Location_Y)\n",
    "        \n",
    "        human_distance_epic.append(soical_obs[0])\n",
    "        soical_reward_epic.append(soical_reward)\n",
    "        \n",
    "        Reward_epic.append(reward)\n",
    "        Energy_epic.append(obs[0])\n",
    "        Speed_epic.append(obs[1])\n",
    "        orientation_epic.append(obs[2])\n",
    "        \n",
    "        episode_reward += reward\n",
    "        \n",
    "        step += 1\n",
    "    \n",
    "    number_epic.append(step)\n",
    "    episode_reward_epic.append(episode_reward)\n",
    "    \n",
    "    episode_reward = 0 \n",
    "    step = 0\n",
    "    done = False    \n",
    "    obs = env.reset()\n",
    "    social_env.setTeststate(obs[0])\n",
    "    soical_obs = [3.6,obs[0]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa0643e",
   "metadata": {},
   "source": [
    "if you want to see any data, plz uncomment it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd81b271",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(Reward_epic)\n",
    "#print(Energy_epic)\n",
    "#print(Speed_epic)\n",
    "#print(Location_X_epic)\n",
    "#print(Location_Y_epic)\n",
    "#print(orientation_epic)\n",
    "#print(number_epic)\n",
    "#print(episode_reward_epic)\n",
    "#print(human_distance_epic)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234854f6",
   "metadata": {},
   "source": [
    "To find the percentage of values in a desire range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c357154c",
   "metadata": {},
   "outputs": [],
   "source": [
    "COUNT = 0\n",
    "for element in orientation_epic:\n",
    "    if element >= 80 and element <= 100:\n",
    "        COUNT += 1\n",
    "    \n",
    "correct = COUNT/len(orientation_epic)\n",
    "print(correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf39527",
   "metadata": {},
   "outputs": [],
   "source": [
    "COUNT = 0\n",
    "for element in number_epic:\n",
    "        COUNT += element\n",
    "    \n",
    "correct = COUNT/len(number_epic)\n",
    "print(correct)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910f7d76",
   "metadata": {},
   "source": [
    "Here is to launch Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529a7271",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "import tensorflow as tf\n",
    "import datetime, os\n",
    "from tensorboard import notebook\n",
    "notebook.list() \n",
    "%tensorboard --logdir=tensorboard --logdir=~/ray_results\n",
    "\n",
    "%tensorboard dev upload --logdir \\ '~/ray_results'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc9ad36",
   "metadata": {},
   "source": [
    "Here is all the plots and animation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66272d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, (ax1, ax2,ax3,ax4) = plt.subplots(1,4)\n",
    "ax1.title.set_text(\"energy reward function behaviour\")\n",
    "ax2.title.set_text(\"speed reward function behaviour\")\n",
    "ax3.title.set_text(\"energy vs speed behaviour\")\n",
    "ax4.title.set_text(\"orientation reward function behaviour\")\n",
    "ax1.plot(Energy_epic, Reward_epic,'bo')\n",
    "ax2.plot(Speed_epic, Reward_epic,'bo')\n",
    "ax3.plot(Energy_epic,Speed_epic,'bo')\n",
    "ax4.plot(orientation_epic,Reward_epic,'bo')\n",
    "ax1.set(xlabel='Energy', ylabel='Reward')\n",
    "ax2.set(xlabel='Speed', ylabel='Reward')\n",
    "ax3.set(xlabel='Energy', ylabel='Speed')\n",
    "ax4.set(xlabel='orientation', ylabel='reward')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ada9757",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot1 = plt. figure(2)\n",
    "plt.plot(range(len(Energy_epic)),Energy_epic)\n",
    "plt.xlabel(\"number of steps\")\n",
    "plt.ylabel(\"energy\")\n",
    "plt.title(\"energy_trained\")\n",
    "\n",
    "\n",
    "plot2 = plt. figure(3)\n",
    "plt.plot(range(len(orientation_epic)),orientation_epic)\n",
    "plt.xlabel(\"number of steps\")\n",
    "plt.ylabel(\"orientation\")\n",
    "plt.title(\"orientation_trained\")\n",
    "\n",
    "plot3 = plt. figure(4)\n",
    "plt.plot(range(len(Reward_epic)),Reward_epic,'bo')\n",
    "plt.xlabel(\"number of steps\")\n",
    "plt.ylabel(\"reward_epic\")\n",
    "plt.title(\"Reward_trained\")\n",
    "\n",
    "plot4 = plt. figure(5)\n",
    "plt.plot(range(len(number_epic)),number_epic,'bo')\n",
    "plt.xlabel(\"number of steps\")\n",
    "plt.ylabel(\"the iterations each episode\")\n",
    "plt.title(\"number_epic_trained\")\n",
    "\n",
    "plot5 = plt. figure(6)\n",
    "plt.plot(range(len(episode_reward_epic)),episode_reward_epic,'bo')\n",
    "plt.xlabel(\"number of steps\")\n",
    "plt.ylabel(\"total reward per episode\")\n",
    "plt.title(\"episode_reward_trained\")\n",
    "plt.ylim(400,2700)\n",
    "\n",
    "plot6 = plt. figure(7)\n",
    "plt.plot(range(len(Speed_epic)),Speed_epic,'bo')\n",
    "plt.xlabel(\"number of steps\")\n",
    "plt.ylabel(\"speed\")\n",
    "plt.title(\"speed_trained\")\n",
    "\n",
    "plot7 = plt. figure(8)\n",
    "plt.hist(Speed_epic, bins=100)\n",
    "plt.gca().set(title='Speed Histogram trained', ylabel='Frequency');\n",
    "\n",
    "plot7 = plt. figure(9)\n",
    "plt.hist(Energy_epic, bins=100)\n",
    "plt.gca().set(title='Energy Histogram trained', ylabel='Frequency');\n",
    "\n",
    "plot7 = plt. figure(10)\n",
    "plt.hist(orientation_epic, bins=100)\n",
    "plt.gca().set(title='orientation Histogram trained', ylabel='Frequency');\n",
    "\n",
    "plot7 = plt. figure(11)\n",
    "plt.hist(Reward_epic, bins=100)\n",
    "plt.gca().set(title='reward Histogram trained', ylabel='Frequency');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0c7e76",
   "metadata": {},
   "source": [
    "Animation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "47194fde",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-22 20:03:14,777\tERROR worker.py:475 -- print_logs: Connection closed by server.\n",
      "2021-10-22 20:03:14,778\tERROR worker.py:1217 -- listen_error_messages_raylet: Connection closed by server.\n",
      "2021-10-22 20:03:14,778\tERROR import_thread.py:88 -- ImportThread: Connection closed by server.\n"
     ]
    }
   ],
   "source": [
    "fig4, ax = plt.subplots()\n",
    "# set the axes limits\n",
    "ax.axis([0,1000,-5,500])\n",
    "# set equal aspect such that the circle is not shown as ellipse\n",
    "ax.set_aspect(\"equal\")\n",
    "# create a point in the axes\n",
    "points = []\n",
    "points.append(ax.plot(500,10, marker=\"o\")[0])\n",
    "points.append(ax.plot(500,10-3.6, marker=\"o\")[0])\n",
    "\n",
    "energy_text = ax.text(0.02, 0.95, '', transform = ax.transAxes)\n",
    "distance_text = ax.text(0.02, 0.90, '', transform = ax.transAxes)\n",
    "orientation_text = ax.text(0.02, 0.85, '', transform = ax.transAxes)\n",
    "speed_text = ax.text(0.02, 0.80, '', transform=ax.transAxes)\n",
    "\n",
    "\n",
    "def update_dot(i):\n",
    "    x = Location_X_epic[i]\n",
    "    y = Location_Y_epic[i]\n",
    "    \n",
    "    X =  Location_X_epic[i]\n",
    "    Y = Location_Y_epic[i] - human_distance_epic[i]*10\n",
    "    \n",
    "    energy_text.set_text('energy = %.2f' % Energy_epic[i])\n",
    "    distance_text.set_text('distance = %.2f' % human_distance_epic[i])\n",
    "    speed_text.set_text('speed = %.2f' % Speed_epic[i])\n",
    "    orientation_text.set_text('orientation = %.2f' % orientation_epic[i])\n",
    "    \n",
    "    points[0].set_data([x],[y])\n",
    "    points[1].set_data([X],[Y])\n",
    "    \n",
    "    return points,energy_text,distance_text,speed_text,orientation_text\n",
    "\n",
    "animition2 = animation.FuncAnimation(fig4,update_dot,interval = 500 ,blit = False)\n",
    "plt.title(\"Simulation\")\n",
    "plt.legend([\"Human\",\"Robot\"])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a9174c",
   "metadata": {},
   "source": [
    "Random agent simulation here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fb78c46c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[32, 62, 90, 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jdu39\\anaconda3\\lib\\site-packages\\gym\\logger.py:34: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float64\u001b[0m\n",
      "  warnings.warn(colorize(\"%s: %s\" % (\"WARN\", msg % args), \"yellow\"))\n"
     ]
    }
   ],
   "source": [
    "Goal = 500\n",
    "Reward_epic_random = []\n",
    "Energy_epic_random = []\n",
    "Speed_epic_random = []\n",
    "orientation_epic_random = []\n",
    "number_epic_random = []\n",
    "Location_X_epic_random = []\n",
    "Location_Y_epic_random = []\n",
    "episode_reward_epic_random = []\n",
    "env = MyEnv({})\n",
    "\n",
    "# run until episode ends\n",
    "episode_reward = 0\n",
    "step = 0\n",
    "done = False\n",
    "obs = env.reset()\n",
    "print(obs)\n",
    "for i in range(50):\n",
    "    while not done:\n",
    "        action = random.randint(0,4)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        person_test = env.getPerson()\n",
    "        Location_X = person_test.getLocation_X()\n",
    "        Location_X_epic_random.append(Location_X)\n",
    "        \n",
    "        Location_Y = person_test.getLocation_Y()\n",
    "        Location_Y_epic_random.append(Location_Y)\n",
    "        \n",
    "        Reward_epic_random.append(reward)\n",
    "        Energy_epic_random.append(obs[0])\n",
    "        Speed_epic_random.append(obs[1])\n",
    "        orientation_epic_random.append(obs[2])\n",
    "        \n",
    "        episode_reward += reward\n",
    "        \n",
    "        step += 1\n",
    "    \n",
    "    number_epic_random.append(step)\n",
    "    episode_reward_epic_random.append(episode_reward)\n",
    "    \n",
    "    episode_reward = 0 \n",
    "    step = 0\n",
    "    done = False    \n",
    "    obs = env.reset()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0d9e0b",
   "metadata": {},
   "source": [
    "Plot and animation and data analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000434fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "COUNT = 0\n",
    "for element in orientation_epic_random:\n",
    "    if element >= 80 and element >= 100:\n",
    "        COUNT += 1\n",
    "    \n",
    "correct = COUNT/len(orientation_epic_random)\n",
    "print(correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6f1c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "COUNT = 0\n",
    "for element in number_epic_random:\n",
    "        COUNT += element\n",
    "    \n",
    "correct = COUNT/len(number_epic_random)\n",
    "print(correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080412cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6c0cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot1 = plt. figure(2)\n",
    "plt.plot(range(len(Energy_epic_random)),Energy_epic_random)\n",
    "plt.xlabel(\"number of steps\")\n",
    "plt.ylabel(\"energy\")\n",
    "plt.title(\"energy_random\")\n",
    "\n",
    "\n",
    "plot2 = plt. figure(3)\n",
    "plt.plot(range(len(orientation_epic_random)),orientation_epic_random)\n",
    "plt.xlabel(\"number of steps\")\n",
    "plt.ylabel(\"orientation\")\n",
    "plt.title(\"orientation_random\")\n",
    "\n",
    "\n",
    "plot3 = plt. figure(4)\n",
    "plt.plot(range(len(Reward_epic_random)),Reward_epic_random,'bo')\n",
    "plt.xlabel(\"number of steps\")\n",
    "plt.ylabel(\"reward_epic\")\n",
    "plt.title(\"Reward_random\")\n",
    "\n",
    "\n",
    "plot4 = plt. figure(5)\n",
    "plt.plot(range(len(number_epic_random)),number_epic_random,'bo')\n",
    "plt.xlabel(\"number of steps\")\n",
    "plt.ylabel(\"the iterations each episode\")\n",
    "plt.title(\"number_epic_random\")\n",
    "\n",
    "\n",
    "plot5 = plt. figure(6)\n",
    "plt.plot(range(len(episode_reward_epic_random)),episode_reward_epic_random,'bo')\n",
    "plt.xlabel(\"number of steps\")\n",
    "plt.ylabel(\"total reward per episode\")\n",
    "plt.title(\"episode_reward_random\")\n",
    "plt.ylim(400,2700)\n",
    "\n",
    "plot1 = plt. figure(7)\n",
    "plt.plot(range(len(Speed_epic_random)),Speed_epic_random,'bo')\n",
    "plt.xlabel(\"number of steps\")\n",
    "plt.ylabel(\"speed\")\n",
    "plt.title(\"speed_random\")\n",
    "\n",
    "plot8 = plt. figure(8)\n",
    "plt.hist(Speed_epic_random, bins=100)\n",
    "plt.gca().set(title='Speed Histogram random', ylabel='Frequency');\n",
    "\n",
    "plot7 = plt. figure(9)\n",
    "plt.hist(Energy_epic_random, bins=100)\n",
    "plt.gca().set(title='Energy Histogram random', ylabel='Frequency');\n",
    "\n",
    "plot7 = plt. figure(10)\n",
    "plt.hist(orientation_epic_random, bins=100)\n",
    "plt.gca().set(title='orientation Histogram random', ylabel='Frequency');\n",
    "\n",
    "plot7 = plt. figure(11)\n",
    "plt.hist(Reward_epic_random, bins=100)\n",
    "plt.gca().set(title='reward Histogram random', ylabel='Frequency');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "009b3564",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig3, ax = plt.subplots()\n",
    "# set the axes limits\n",
    "ax.axis([0,1000,0,1000])\n",
    "# set equal aspect such that the circle is not shown as ellipse\n",
    "ax.set_aspect(\"equal\")\n",
    "# create a point in the axes\n",
    "points = []\n",
    "points.append(ax.plot(500,10, marker=\"o\")[0])\n",
    "points.append(ax.plot(500,10-3.6, marker=\"o\")[0])\n",
    "\n",
    "energy_text = ax.text(0.02, 0.95, '', transform = ax.transAxes)\n",
    "distance_text = ax.text(0.02, 0.90, '', transform = ax.transAxes)\n",
    "orientation_text = ax.text(0.02, 0.85, '', transform = ax.transAxes)\n",
    "speed_text = ax.text(0.02, 0.80, '', transform=ax.transAxes)\n",
    "\n",
    "def update_dot(i):\n",
    "    x = Location_X_epic_random[i]\n",
    "    y = Location_Y_epic_random[i]\n",
    "    \n",
    "    X =  Location_X_epic_random[i]\n",
    "    Y = Location_Y_epic_random[i] - 2.5*10\n",
    "    \n",
    "    energy_text.set_text('energy = %.2f' % Energy_epic_random[i])\n",
    "    speed_text.set_text('speed = %.2f' % Speed_epic_random[i])\n",
    "    orientation_text.set_text('orientation = %.2f' % orientation_epic_random[i])\n",
    "    \n",
    "    points[0].set_data([x],[y])\n",
    "    points[1].set_data([X],[Y])\n",
    "    \n",
    "    return points,energy_text,speed_text,orientation_text\n",
    "\n",
    "plt.title(\"Simulation\")\n",
    "plt.legend([\"Human\",\"Robot\"])\n",
    "animition1 = animation.FuncAnimation(fig3,update_dot,interval = 500 ,blit = False)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8323742",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
